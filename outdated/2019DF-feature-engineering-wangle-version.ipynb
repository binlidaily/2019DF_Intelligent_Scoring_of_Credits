{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "# from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.pyplot as plt\n",
    "# show more columns with trian_df.describe()\n",
    "pd.set_option('display.max_columns', 50)\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf-8')\n",
    "import time\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False # 用来正常显示负号\n",
    "# plt.plot([-1,2,-5,3])\n",
    "# plt.title(u'中文',fontproperties=myfont)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data_dir = '../dataset/DataFountain2019-消费者人群画像/'\n",
    "train_df = pd.read_csv(data_dir + 'train_dataset.csv')\n",
    "test_df = pd.read_csv(data_dir + 'test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_process(data_df):\n",
    "    transform_value_feats = ['用户年龄', '用户网龄（月）', '当月通话交往圈人数', '近三个月月均商场出现次数',\n",
    "                            '当月网购类应用使用次数', '当月物流快递类应用使用次数', '当月金融理财类应用使用总次数', \n",
    "                             '当月视频播放类应用使用次数', '当月飞机类应用使用次数', '当月火车类应用使用次数', \n",
    "                             '当月旅游资讯类应用使用次数']\n",
    "    bill_feats = ['缴费用户最近一次缴费金额（元）', '用户近6个月平均消费值（元）','用户账单当月总费用（元）', \n",
    "                   '用户当月账户余额（元）']\n",
    "    log_feats = ['当月网购类应用使用次数', '当月金融理财类应用使用总次数', '当月视频播放类应用使用次数']\n",
    "    \n",
    "    # 处理极小或极大的离散点\n",
    "    for col in transform_value_feats + bill_feats:\n",
    "        up_limit = np.percentile(data_df[col].values, 99.9) # 99.9%分位数\n",
    "        low_limit = np.percentile(data_df[col].values, 0.1) # 0.1%分位数\n",
    "        data_df.loc[data_df[col] > up_limit, col] = up_limit\n",
    "        data_df.loc[data_df[col] < low_limit, col] = low_limit\n",
    "    \n",
    "    # 解决正太分布左偏的情况，取对数\n",
    "    for col in bill_feats + log_feats:\n",
    "        data_df[col] = data_df[col].map(lambda x : np.log1p(x))\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "# run\n",
    "train_df = base_process(train_df)\n",
    "test_df = base_process(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #2 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my methods to create new features\n",
    "def create_features(data_df):\n",
    "    # 异常值处理\n",
    "    ## 对年龄异常值取\n",
    "    data_df.loc[data_df['用户年龄'] == 0, '用户年龄'] = data_df['用户年龄'].mode() # 线下测试，众数比平均数好\n",
    "    ## 用户话费敏感度处理\n",
    "    data_df.loc[data_df['用户话费敏感度'] == 0, '用户话费敏感度'] = data_df['用户话费敏感度'].mode()\n",
    "    \n",
    "\n",
    "    # 用户费用相关特征\n",
    "    ## 不同的充值路径\n",
    "    data_df['不同充值途径'] = 0\n",
    "    data_df.loc[(data_df['缴费用户最近一次缴费金额（元）'] % 10 == 0) & \n",
    "                      (data_df['缴费用户最近一次缴费金额（元）'] != 0), '不同充值途径'] = 1\n",
    "    ## 费用稳定性\n",
    "    data_df['当前费用稳定性'] = data_df['用户账单当月总费用（元）'] / (data_df['用户近6个月平均消费值（元）'] + 1)\n",
    "   \n",
    "    # 构造 ratio 比例特征\n",
    "    ## '缴费用户最近一次缴费金额（元）'/'用户当月账户余额（元）'\n",
    "    data_df['充值_余额_比例'] = data_df['缴费用户最近一次缴费金额（元）'] / (data_df['用户当月账户余额（元）'] + 1)\n",
    "    ## 用户账单当月总费用/当月账户余额\n",
    "    data_df['月费_余额_比例'] = data_df['用户账单当月总费用（元）'] / (data_df['用户当月账户余额（元）'] + 1)\n",
    "    # '用户账单当月总费用（元）'/ '缴费用户最近一次缴费金额（元）'\n",
    "    data_df['月费_缴费_比例'] = data_df['用户账单当月总费用（元）'] / (data_df['缴费用户最近一次缴费金额（元）'] + 1)\n",
    "    ## '用户近6个月平均消费值（元）'/ '缴费用户最近一次缴费金额（元）'\n",
    "    data_df['均费_缴费_比例'] = data_df['用户近6个月平均消费值（元）'] / (data_df['缴费用户最近一次缴费金额（元）'] + 1)\n",
    "    ## '用户近6个月平均消费值（元）' / \n",
    "    data_df['均费_月费_比例'] = data_df['用户近6个月平均消费值（元）'] / (data_df['用户账单当月总费用（元）'] + 1)\n",
    "    \n",
    "    ## 用户上网年龄\n",
    "    data_df['用户上网年龄'] = data_df['用户年龄'] - data_df['用户网龄（月）']\n",
    "#     ## '用户网龄（月）'/'用户年龄', '用户年龄'/ '用户网龄（月）'不是很好算出来，毕竟是个大数\n",
    "#     data_df['网龄_年龄_比例'] = data_df['用户网龄（月）'] / (data_df['用户年龄'] + 1)\n",
    "    \n",
    "    \n",
    "    # 构造加减特征\n",
    "    data_df['缴费金额是否能覆盖当月账单'] = data_df['缴费用户最近一次缴费金额（元）'] - data_df['用户账单当月总费用（元）']\n",
    "    data_df['最近一次缴费是否超过平均消费额'] = data_df['缴费用户最近一次缴费金额（元）'] - data_df['用户近6个月平均消费值（元）']\n",
    "    data_df['当月账单是否超过平均消费额'] = data_df['用户账单当月总费用（元）'] - data_df['用户近6个月平均消费值（元）']\n",
    "    \n",
    "    # 对 bool 特征进行简单构造\n",
    "    data_df['是否去过高档商场'] = data_df['当月是否到过福州山姆会员店'] + data_df['当月是否逛过福州仓山万达']\n",
    "    ## 检查后发现结果为2的比较稀少，于是将1、2都归到1中\n",
    "    data_df['是否去过高档商场'] = data_df['是否去过高档商场'].map(lambda x : 1 if x >= 1 else 0)\n",
    "    \n",
    "    \n",
    "    data_df['是否_商场_电影'] = data_df['是否去过高档商场'] * data_df['当月是否看电影']\n",
    "    data_df['是否_商场_旅游'] = data_df['是否去过高档商场'] * data_df['当月是否景点游览']\n",
    "    data_df['是否_商场_体育馆'] = data_df['是否去过高档商场'] * data_df['当月是否体育场馆消费']\n",
    "    data_df['是否_电影_体育馆'] = data_df['当月是否看电影'] * data_df['当月是否体育场馆消费']\n",
    "    data_df['是否_电影_旅游'] = data_df['当月是否看电影'] * data_df['当月是否景点游览']\n",
    "    data_df['是否_旅游_体育馆'] = data_df['当月是否景点游览'] * data_df['当月是否体育场馆消费']\n",
    "    \n",
    "    data_df['是否_商场_旅游_体育馆'] = data_df['是否去过高档商场'] * data_df['当月是否景点游览'] * data_df['当月是否体育场馆消费']\n",
    "    data_df['是否_商场_电影_体育馆'] = data_df['是否去过高档商场'] * data_df['当月是否看电影'] * data_df['当月是否体育场馆消费']\n",
    "    data_df['是否_商场_电影_旅游'] = data_df['是否去过高档商场'] * data_df['当月是否看电影'] * data_df['当月是否景点游览']\n",
    "    data_df['是否_体育馆_电影_旅游'] = data_df['当月是否体育场馆消费'] * data_df['当月是否看电影'] * data_df['当月是否景点游览']\n",
    "    \n",
    "    data_df['是否_商场_体育馆_电影_旅游'] = data_df['是否去过高档商场'] * data_df['当月是否体育场馆消费'] * \\\n",
    "                                        data_df['当月是否看电影'] * data_df['当月是否景点游览']\n",
    "    \n",
    "#     # 杰少特征参考\n",
    "#     data_df['次数'] = data_df['当月网购类应用使用次数'] +  data_df['当月物流快递类应用使用次数'] +  data_df['当月金融理财类应用使用总次数'] + \\\n",
    "#                 data_df['当月视频播放类应用使用次数'] + data_df['当月飞机类应用使用次数'] + data_df['当月火车类应用使用次数'] + \\\n",
    "#                 data_df['当月旅游资讯类应用使用次数']  + 1\n",
    "\n",
    "#     for col in ['当月金融理财类应用使用总次数', '当月旅游资讯类应用使用次数']: # 这两个比较积极向上一点\n",
    "#         data_df[col + '百分比'] = data_df[col] / data_df['次数'] \n",
    "\n",
    "#     data_df['当月通话人均话费'] = data_df['用户账单当月总费用（元）'] / (data_df['当月通话交往圈人数'] + 1)\n",
    "\n",
    "#     data_df['上个月费用'] = data_df['用户当月账户余额（元）'] + data_df['用户账单当月总费用（元）']\n",
    "\n",
    "#     data_df['近似总消费'] = data_df['用户近6个月平均消费值（元）'] * data_df['用户网龄（月）'] / 12\n",
    "    \n",
    "    \n",
    "    return data_df\n",
    "\n",
    "# run\n",
    "train_df = create_features(train_df)\n",
    "test_df = create_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #3 Single Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless features\n",
    "# print train_df.columns, len(train_df.columns)\n",
    "drop_cols = ['用户编码', '是否黑名单客户']\n",
    "\n",
    "X = train_df.drop(drop_cols + ['信用分'], axis=1)\n",
    "X_submit = test_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimension Reduction 降维\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=600)\n",
    "# pca.fit(X)\n",
    "# X = pca.fit_transform(X)\n",
    "# pca.fit(X_submit)\n",
    "# X_submit = pca.fit_transform(X_submit)\n",
    "# print X.shape, X_submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-cv\n",
    "N_FOLDS = 5\n",
    "y = train_df['信用分']\n",
    "kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=2019)\n",
    "kf = kfold.split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2198]\tvalid_0's l1: 14.7833\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2360]\tvalid_0's l1: 14.6713\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2225]\tvalid_0's l1: 14.7905\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2617]\tvalid_0's l1: 14.4921\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2521]\tvalid_0's l1: 14.7948\n",
      "('cv score for valid is: ', 0.06366834321147859)\n"
     ]
    }
   ],
   "source": [
    "# LightGBM: GBDT\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 5,\n",
    "    'lambda_l1': 0,\n",
    "    'lambda_l2': 2.5,\n",
    "    'nthread': 4\n",
    "}\n",
    "\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:100].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# 0--------------------------------------1\n",
    "\n",
    "\n",
    "\n",
    "# process the k-cv\n",
    "cv_pred = np.zeros(test_df.shape[0])\n",
    "valid_best_l2_all = 0\n",
    "\n",
    "feature_importance_df = pd.DataFrame()\n",
    "count = 0\n",
    "for i, (train_idx, test_idx) in enumerate(kf):\n",
    "    print('fold: ',i, ' training')\n",
    "    X_train, X_test, y_train, y_test = X.iloc[train_idx, :], X.iloc[test_idx, :], y.iloc[train_idx], y.iloc[test_idx]\n",
    "#     X_train, X_test, y_train, y_test = X[train_idx, :], X[test_idx, :], y[train_idx], y[test_idx]\n",
    "    data_train = lgb.Dataset(X_train, y_train)\n",
    "    data_test = lgb.Dataset(X_test, y_test)\n",
    "    lgb_model = lgb.train(params, data_train, num_boost_round=10000, valid_sets=data_test, \n",
    "                          verbose_eval=-1, early_stopping_rounds=50)\n",
    "    cv_pred += lgb_model.predict(X_submit, num_iteration=lgb_model.best_iteration)\n",
    "    valid_best_l2_all += lgb_model.best_score['valid_0']['l1']\n",
    "    \n",
    "#     fold_importance_df = pd.DataFrame()\n",
    "#     fold_importance_df[\"feature\"] = list(unicode(X_train.columns))\n",
    "#     fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain', iteration=lgb_model.best_iteration)\n",
    "#     fold_importance_df[\"fold\"] = count + 1\n",
    "#     feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "cv_pred /= N_FOLDS\n",
    "valid_best_l2_all /= N_FOLDS\n",
    "print('cv score for valid is: ', 1 / (1 + valid_best_l2_all))\n",
    "\n",
    "# show the importance of features\n",
    "# display_importances(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06371758445225165"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.06364782116275812\n",
    "# 0.063714735700162436 ———— 去掉'年龄_网龄_比例'特征后\n",
    "# 0.063735568491715147 ———— 众数填充为零的年龄（平均数填充结果：0.063640145348041702，较差）\n",
    "# 0.063666002670369704 ———— 构造加减特征\n",
    "# 0.063684663314497278 ———— 构造'是否去过高档商场'\n",
    "# 0.063714215150177056 ———— 用众数替代‘用户话费敏感度’中的 0 值（平均数填充结果：0.063683775050515118，较差）\n",
    "# 0.0637013629544181 ———— 加上了base_process里面对左偏分布的处理\n",
    "\n",
    "0.06368864913531594\n",
    "\n",
    "0.06371758445225165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # XGBoost\n",
    "# import xgboost as xgb\n",
    "# xgb_params={'eta': 0.005, 'max_depth': 10, 'subsample': 0.8, 'colsample_bytree': 0.8, \n",
    "#           'objective': 'reg:linear', 'eval_metric': 'mae', 'silent': True, 'nthread': 8}\n",
    "# from sklearn.model_selection import KFold\n",
    "# cv_pred_allxgb=0\n",
    "# en_amount=3\n",
    "# oof_xgb1=np.zeros(len(train_data))\n",
    "# prediction_xgb1=np.zeros(len(test_data))\n",
    "# for seed in range(en_amount):\n",
    "#     NFOLDS=5\n",
    "#     train_label=train_data['信用分']\n",
    "#     kfold=KFold(n_splits=NFOLDS, shuffle=True, random_state=seed+2019)\n",
    "#     kf=kfold.split(train_data,train_label)\n",
    "    \n",
    "#     train_data_use = train_data.drop(['用户编码','信用分'], axis=1)\n",
    "#     test_data_use = test_data.drop(['用户编码'], axis=1)\n",
    "    \n",
    "#     cv_pred = np.zeros(test_data.shape[0])\n",
    "#     valid_best_l2_all = 0\n",
    "    \n",
    "#     feature_importance_df = pd.DataFrame()\n",
    "#     count = 0\n",
    "    \n",
    "#     for i, (train_fold, validate) in enumerate(kf):\n",
    "#         print('fold: ',i, ' training')\n",
    "#         X_train, X_validate, label_train, label_validate = train_data_use.iloc[train_fold, :], train_data_use.iloc[validate, :], train_label[train_fold], train_label[validate]\n",
    "#         dtrain = xgb.DMatrix(X_train, label_train)\n",
    "#         dvalid = xgb.DMatrix(X_validate, label_validate)\n",
    "#         watchlist = [(dtrain, 'train'), (dvalid, 'valid_data')]\n",
    "#         bst = xgb.train(dtrain=dtrain, num_boost_round=10000, evals=watchlist, early_stopping_rounds=100, verbose_eval=300, params=xgb_params)\n",
    "#         cv_pred += bst.predict(xgb.DMatrix(test_data_use), ntree_limit=bst.best_ntree_limit)\n",
    "#         oof_xgb1[validate]=bst.predict(xgb.DMatrix(X_validate),ntree_limit=bst.best_ntree_limit)\n",
    "#         prediction_xgb1+=bst.predict(xgb.DMatrix(test_data_use),ntree_limit=bst.best_ntree_limit)/kfold.n_splits\n",
    "#         count += 1\n",
    "        \n",
    "#     cv_pred /= NFOLDS\n",
    "#     cv_pred_allxgb+=cv_pred\n",
    "# cv_pred_allxgb /= en_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Logistical Regression 逻辑斯特回归\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# # process the k-cv\n",
    "# cv_pred = np.zeros(test_df.shape[0])\n",
    "# valid_best_l2_all = 0\n",
    "\n",
    "# feature_importance_df = pd.DataFrame()\n",
    "# count = 0\n",
    "# for i, (train_idx, test_idx) in enumerate(kf):\n",
    "#     print('fold: ',i, ' training')\n",
    "#     X_train, X_test, y_train, y_test = X.iloc[train_idx, :], X.iloc[test_idx, :], y.iloc[train_idx], y.iloc[test_idx]\n",
    "# #     X_train, X_test, y_train, y_test = X[train_idx, :], X[test_idx, :], y[train_idx], y[test_idx]\n",
    "#     clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "#     valid_best_l2_all += mean_absolute_error(clf.predict(X_test), y_test)\n",
    "#     cv_pred += clf.predict(X_submit)\n",
    "\n",
    "# cv_pred /= N_FOLDS\n",
    "# print('cv score for valid is: ', 1/(1+valid_best_l2_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #4 Single model submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = test_df[['用户编码']]\n",
    "submit_df['score'] = cv_pred\n",
    "submit_df.columns = ['id', 'score']\n",
    "\n",
    "# 信用分都是整数\n",
    "submit_df['score'] = submit_df['score'].apply(lambda x: int(np.round(x)))\n",
    "submit_df.to_csv('./submission/baseline_2019-03-10T08:47:26_0.06363332257662933.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a4651f98c82948b186bdcdc8108381b4</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aeb10247db4e4d67b2550bbc42ff9827</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5af23a1e0e77410abb25e9a7eee510aa</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43c64379d3c24a15b8478851b22049e4</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f1687f3b8a6f4910bd0b13eb634056e2</td>\n",
       "      <td>513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>52795d470db4478584f6c92f66af0294</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0d758e1b10cc4f618dda9f87fc948068</td>\n",
       "      <td>523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b05585f9635245f282bf2cffd3c5773c</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>acde81ca14eb429a983bec773e16098c</td>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1f78ee310e9d48449b8d5a1fd1537286</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id  score\n",
       "0  a4651f98c82948b186bdcdc8108381b4    534\n",
       "1  aeb10247db4e4d67b2550bbc42ff9827    441\n",
       "2  5af23a1e0e77410abb25e9a7eee510aa    524\n",
       "3  43c64379d3c24a15b8478851b22049e4    525\n",
       "4  f1687f3b8a6f4910bd0b13eb634056e2    513\n",
       "5  52795d470db4478584f6c92f66af0294    516\n",
       "6  0d758e1b10cc4f618dda9f87fc948068    523\n",
       "7  b05585f9635245f282bf2cffd3c5773c    489\n",
       "8  acde81ca14eb429a983bec773e16098c    453\n",
       "9  1f78ee310e9d48449b8d5a1fd1537286    446"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### —————————————————————————————————————————————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #5 Model ensembling Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless features\n",
    "drop_cols = ['用户编码', '是否黑名单客户']\n",
    "\n",
    "X = train_df.drop(drop_cols + ['信用分'], axis=1)\n",
    "X_submit = test_df.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2324]\tvalid_0's l1: 14.7854\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2317]\tvalid_0's l1: 14.6964\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3255]\tvalid_0's l1: 14.7701\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3321]\tvalid_0's l1: 14.477\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2141]\tvalid_0's l1: 14.8283\n",
      "('cv score for valid is: ', 0.06364782116275812)\n",
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2324]\tvalid_0's l1: 14.7854\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2317]\tvalid_0's l1: 14.6964\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3255]\tvalid_0's l1: 14.7701\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3321]\tvalid_0's l1: 14.477\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2141]\tvalid_0's l1: 14.8283\n",
      "('cv score for valid is: ', 0.063647821162758106)\n",
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2324]\tvalid_0's l1: 14.7854\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2317]\tvalid_0's l1: 14.6964\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3255]\tvalid_0's l1: 14.7701\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[3321]\tvalid_0's l1: 14.477\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2141]\tvalid_0's l1: 14.8283\n",
      "('cv score for valid is: ', 0.063647821162758106)\n"
     ]
    }
   ],
   "source": [
    "# LightGBM params\n",
    "params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l1',\n",
    "    'metric': 'mae',\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 5,\n",
    "    'lambda_l2': 5, 'lambda_l1': 0\n",
    "}\n",
    "\n",
    "# process the k-cv\n",
    "cv_pred_all = 0\n",
    "en_amount = 3\n",
    "for seed in range(en_amount):\n",
    "    # k-cv\n",
    "    N_FOLDS = 5\n",
    "    y = train_df['信用分']\n",
    "    kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=2019)\n",
    "    kf = kfold.split(X, y)\n",
    "\n",
    "    # process the k-cv\n",
    "    cv_pred = np.zeros(test_df.shape[0])\n",
    "    valid_best_l2_all = 0\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for i, (train_idx, test_idx) in enumerate(kf):\n",
    "        print('fold: ',i, ' training')\n",
    "        X_train, X_test, y_train, y_test = X.iloc[train_idx, :], X.iloc[test_idx, :], y.iloc[train_idx], y.iloc[test_idx]\n",
    "#         X_train, X_test, y_train, y_test = X[train_idx, :], X[test_idx, :], y[train_idx], y[test_idx]\n",
    "        data_train = lgb.Dataset(X_train, y_train)\n",
    "        data_test = lgb.Dataset(X_test, y_test)\n",
    "        lgb_model = lgb.train(params, data_train, num_boost_round=10000, valid_sets=data_test, \n",
    "                              verbose_eval=-1, early_stopping_rounds=50)\n",
    "        cv_pred += lgb_model.predict(X_submit, num_iteration=lgb_model.best_iteration)\n",
    "        valid_best_l2_all += lgb_model.best_score['valid_0']['l1']\n",
    "\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"feature\"] = list(X_train.columns)\n",
    "#         fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain', iteration=lgb_model.best_iteration)\n",
    "#         fold_importance_df[\"fold\"] = count + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        count += 1\n",
    "\n",
    "    cv_pred /= N_FOLDS\n",
    "    valid_best_l2_all /= N_FOLDS\n",
    "    print('cv score for valid is: ', 1/(1+valid_best_l2_all))\n",
    "    \n",
    "    cv_pred_all += cv_pred\n",
    "\n",
    "# avg cv_pred_all\n",
    "cv_pred_all = cv_pred_all / en_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2019]\tvalid_0's l1: 14.8025\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2143]\tvalid_0's l1: 14.6985\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2732]\tvalid_0's l1: 14.8106\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2004]\tvalid_0's l1: 14.5162\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2499]\tvalid_0's l1: 14.7874\n",
      "('cv score for valid is: ', 0.063600933726241329)\n",
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2019]\tvalid_0's l1: 14.8025\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2143]\tvalid_0's l1: 14.6985\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2732]\tvalid_0's l1: 14.8106\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2004]\tvalid_0's l1: 14.5162\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2499]\tvalid_0's l1: 14.7874\n",
      "('cv score for valid is: ', 0.063600933726241329)\n",
      "('fold: ', 0, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2019]\tvalid_0's l1: 14.8025\n",
      "('fold: ', 1, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2143]\tvalid_0's l1: 14.6985\n",
      "('fold: ', 2, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2732]\tvalid_0's l1: 14.8106\n",
      "('fold: ', 3, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2004]\tvalid_0's l1: 14.5162\n",
      "('fold: ', 4, ' training')\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[2499]\tvalid_0's l1: 14.7874\n",
      "('cv score for valid is: ', 0.063600933726241343)\n"
     ]
    }
   ],
   "source": [
    "#para\n",
    "params2 = {\n",
    "    'learning_rate': 0.01,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression_l2',\n",
    "    'metric': 'mae',\n",
    "    'feature_fraction': 0.6,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 2,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'max_depth': 5,\n",
    "    'lambda_l2': 5, 'lambda_l1': 0,'nthread': 8,\n",
    "    'seed': 89\n",
    "}\n",
    "\n",
    "# process the k-cv\n",
    "cv_pred_all2 = 0\n",
    "en_amount = 3\n",
    "for seed in range(en_amount):\n",
    "    # k-cv\n",
    "    N_FOLDS = 5\n",
    "    y = train_df['信用分']\n",
    "    kfold = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=2019)\n",
    "    kf = kfold.split(X, y)\n",
    "\n",
    "    # process the k-cv\n",
    "    cv_pred = np.zeros(test_df.shape[0])\n",
    "    valid_best_l2_all = 0\n",
    "\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    count = 0\n",
    "    for i, (train_idx, test_idx) in enumerate(kf):\n",
    "        print('fold: ',i, ' training')\n",
    "        X_train, X_test, y_train, y_test = X.iloc[train_idx, :], X.iloc[test_idx, :], y.iloc[train_idx], y.iloc[test_idx]\n",
    "#         X_train, X_test, y_train, y_test = X[train_idx, :], X[test_idx, :], y[train_idx], y[test_idx]\n",
    "        data_train = lgb.Dataset(X_train, y_train)\n",
    "        data_test = lgb.Dataset(X_test, y_test)\n",
    "        lgb_model = lgb.train(params2, data_train, num_boost_round=10000, valid_sets=data_test, \n",
    "                              verbose_eval=-1, early_stopping_rounds=50)\n",
    "        cv_pred += lgb_model.predict(X_submit, num_iteration=lgb_model.best_iteration)\n",
    "        valid_best_l2_all += lgb_model.best_score['valid_0']['l1']\n",
    "\n",
    "#         fold_importance_df = pd.DataFrame()\n",
    "#         fold_importance_df[\"feature\"] = list(X_train.columns)\n",
    "#         fold_importance_df[\"importance\"] = lgb_model.feature_importance(importance_type='gain', iteration=lgb_model.best_iteration)\n",
    "#         fold_importance_df[\"fold\"] = count + 1\n",
    "#         feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        count += 1\n",
    "\n",
    "    cv_pred /= N_FOLDS\n",
    "    valid_best_l2_all /= N_FOLDS\n",
    "    print('cv score for valid is: ', 1/(1+valid_best_l2_all))\n",
    "    \n",
    "    cv_pred_all2 += cv_pred\n",
    "\n",
    "# avg cv_pred_all\n",
    "cv_pred_all2 = cv_pred_all2 / en_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## #6 Model Ensembling Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = test_df[['用户编码']]\n",
    "submit_df['score'] = (cv_pred_all + cv_pred_all2) / 2\n",
    "submit_df.columns = ['id', 'score']\n",
    "submit_df['score1'] = cv_pred_all\n",
    "submit_df['score2'] = cv_pred_all2\n",
    "\n",
    "# int\n",
    "submit_df['score'] = submit_df['score'].apply(lambda x: int(np.round(x)))\n",
    "submit_df[['id','score']].to_csv('./submission/model_ensemble_baseline_2019-03-07T09:40:35_0.063600933726241343.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
